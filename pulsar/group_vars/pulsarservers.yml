galaxy_server_hostname: "hyperion.cac.cornell.edu" # Important!!!
# Put your Galaxy server's fully qualified domain name (FQDN) (or the FQDN of the RabbitMQ server) above.

# Path to install pulsar and all subsequent dependencies
pulsar_root: /storage/group/bfp2/default/00_pughlab/pulsar

pulsar_pip_install: true
pulsar_pycurl_ssl_library: openssl
pulsar_systemd: true
pulsar_systemd_runner: webless
pulsar_systemd_environment: [ DRMAA_LIBRARY_PATH=/storage/group/bfp2/default/00_pughlab/pulsar/slurm-drmaa/lib/libdrmaa.so.1 ]

pulsar_create_user: false
# Set userID for service account
pulsar_user: {name: other_5f6ad95074eb4e, shell: /bin/bash}
pulsar_privsep_user: other_5f6ad95074eb4e
pulsar_group: bfp2_collab

pulsar_optional_dependencies:
  - pyOpenSSL
  # For remote transfers initiated on the Pulsar end rather than the Galaxy end
  - pycurl
  # drmaa required if connecting to an external DRM using it.
  - drmaa
  # kombu needed if using a message queue
  - kombu
  # amqp 5.0.3 changes behaviour in an unexpected way, pin for now.
  - 'amqp==5.0.2'
  # psutil and pylockfile are optional dependencies but can make Pulsar
  # more robust in small ways.
  - psutil

pulsar_yaml_config:
  staging_directory: "{{ pulsar_staging_dir }}"
  persistence_directory: "{{ pulsar_persistence_dir }}"
  tool_dependency_dir: "{{ pulsar_dependencies_dir }}"
  # The following are the settings for the pulsar server to contact the message queue with related timeouts etc.
  # # JS2 destination
  message_queue_url: "pyamqp://galaxy_iu:{{ rabbitmq_users_password.iupulsar }}@{{ galaxy_server_hostname }}:5671//pulsar/galaxy_iu?ssl=1"
  # PSU destination
  # message_queue_url: "pyamqp://galaxy_psu:{{ rabbitmq_users_password.psupulsar }}@{{ galaxy_server_hostname }}:5671//pulsar/galaxy_psu?ssl=1"

  min_polling_interval: 0.5
  amqp_publish_retry: True
  amqp_publish_retry_max_retries: 5
  amqp_publish_retry_interval_start: 10
  amqp_publish_retry_interval_step: 10
  amqp_publish_retry_interval_max: 60
  # We also need to create the dependency resolvers configuration so pulsar knows how to find and install dependencies
  # for the tools we ask it to run. The simplest method which covers 99% of the use cases is to use conda auto installs
  # similar to how Galaxy works.
  dependency_resolution:
    resolvers:
      - type: conda
        auto_init: true
        auto_install: true
  managers:
    production:
      type: queued_drmaa
      #type: queued_python
      #native_specification: "--account=open --time=8:00:00 --nodes=1 --cpus-per-task=4 --mem=8GB"
    high_core:
      #type: queued_drmaa
      type: queued_python
      #native_specification: "--account=open --time=8:00:00 --nodes=1 --cpus-per-task=12 --mem=72GB"
    med_core:
      #type: queued_drmaa
      type: queued_python
      #native_specification: "--account=open --time=8:00:00 --nodes=1 --cpus-per-task=4 --mem=28GB"
    low_core:
      #type: queued_drmaa
      type: queued_python
      #native_specification: "--account=open --time=8:00:00 --nodes=1 --cpus-per-task=1 --mem=8GB"
    _default_:
      #type: queued_drmaa
      type: queued_python
      #native_specification: "--account=open --time=8:00:00 --nodes=1 --cpus-per-task=4 --mem=28GB"

# Pulsar should use the same job metrics plugins as Galaxy. This will automatically set `job_metrics_config_file` in
# `pulsar_yaml_config` and create `{{ pulsar_config_dir }}/job_metrics_conf.yml`.
pulsar_job_metrics_plugins: "{{ galaxy_job_metrics_plugins }}"

# Slurm
slurm_roles: ['controller', 'exec'] # Which roles should the machine play? exec are execution hosts.
slurm_nodes:
- name: localhost # Name of our host
  CPUs: 2         # Here you would need to figure out how many cores your machine has. For this training we will use 2 but in real life, look at `htop` or similar.
slurm_config:
  SlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2
  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node
